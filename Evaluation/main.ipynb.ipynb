{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuGSNTDTnqdZ"
      },
      "source": [
        "# Niqe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3ZIJgYJn-HM"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def cubic(x):\n",
        "    \"\"\"cubic function used for calculate_weights_indices.\"\"\"\n",
        "    absx = torch.abs(x)\n",
        "    absx2 = absx**2\n",
        "    absx3 = absx**3\n",
        "    return (1.5 * absx3 - 2.5 * absx2 + 1) * (\n",
        "        (absx <= 1).type_as(absx)) + (-0.5 * absx3 + 2.5 * absx2 - 4 * absx +\n",
        "                                      2) * (((absx > 1) *\n",
        "                                             (absx <= 2)).type_as(absx))\n",
        "\n",
        "\n",
        "def calculate_weights_indices(in_length, out_length, scale, kernel,\n",
        "                              kernel_width, antialiasing):\n",
        "    \"\"\"Calculate weights and indices, used for imresize function.\n",
        "\n",
        "    Args:\n",
        "        in_length (int): Input length.\n",
        "        out_length (int): Output length.\n",
        "        scale (float): Scale factor.\n",
        "        kernel_width (int): Kernel width.\n",
        "        antialisaing (bool): Whether to apply anti-aliasing when downsampling.\n",
        "    \"\"\"\n",
        "\n",
        "    if (scale < 1) and antialiasing:\n",
        "        # Use a modified kernel (larger kernel width) to simultaneously\n",
        "        # interpolate and antialias\n",
        "        kernel_width = kernel_width / scale\n",
        "\n",
        "    # Output-space coordinates\n",
        "    x = torch.linspace(1, out_length, out_length)\n",
        "\n",
        "    # Input-space coordinates. Calculate the inverse mapping such that 0.5\n",
        "    # in output space maps to 0.5 in input space, and 0.5 + scale in output\n",
        "    # space maps to 1.5 in input space.\n",
        "    u = x / scale + 0.5 * (1 - 1 / scale)\n",
        "\n",
        "    # What is the left-most pixel that can be involved in the computation?\n",
        "    left = torch.floor(u - kernel_width / 2)\n",
        "\n",
        "    # What is the maximum number of pixels that can be involved in the\n",
        "    # computation?  Note: it's OK to use an extra pixel here; if the\n",
        "    # corresponding weights are all zero, it will be eliminated at the end\n",
        "    # of this function.\n",
        "    p = math.ceil(kernel_width) + 2\n",
        "\n",
        "    # The indices of the input pixels involved in computing the k-th output\n",
        "    # pixel are in row k of the indices matrix.\n",
        "    indices = left.view(out_length, 1).expand(out_length, p) + torch.linspace(\n",
        "        0, p - 1, p).view(1, p).expand(out_length, p)\n",
        "\n",
        "    # The weights used to compute the k-th output pixel are in row k of the\n",
        "    # weights matrix.\n",
        "    distance_to_center = u.view(out_length, 1).expand(out_length, p) - indices\n",
        "\n",
        "    # apply cubic kernel\n",
        "    if (scale < 1) and antialiasing:\n",
        "        weights = scale * cubic(distance_to_center * scale)\n",
        "    else:\n",
        "        weights = cubic(distance_to_center)\n",
        "\n",
        "    # Normalize the weights matrix so that each row sums to 1.\n",
        "    weights_sum = torch.sum(weights, 1).view(out_length, 1)\n",
        "    weights = weights / weights_sum.expand(out_length, p)\n",
        "\n",
        "    # If a column in weights is all zero, get rid of it. only consider the\n",
        "    # first and last column.\n",
        "    weights_zero_tmp = torch.sum((weights == 0), 0)\n",
        "    if not math.isclose(weights_zero_tmp[0], 0, rel_tol=1e-6):\n",
        "        indices = indices.narrow(1, 1, p - 2)\n",
        "        weights = weights.narrow(1, 1, p - 2)\n",
        "    if not math.isclose(weights_zero_tmp[-1], 0, rel_tol=1e-6):\n",
        "        indices = indices.narrow(1, 0, p - 2)\n",
        "        weights = weights.narrow(1, 0, p - 2)\n",
        "    weights = weights.contiguous()\n",
        "    indices = indices.contiguous()\n",
        "    sym_len_s = -indices.min() + 1\n",
        "    sym_len_e = indices.max() - in_length\n",
        "    indices = indices + sym_len_s - 1\n",
        "    return weights, indices, int(sym_len_s), int(sym_len_e)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def imresize(img, scale, antialiasing=True):\n",
        "    \"\"\"imresize function same as MATLAB.\n",
        "\n",
        "    It now only supports bicubic.\n",
        "    The same scale applies for both height and width.\n",
        "\n",
        "    Args:\n",
        "        img (Tensor | Numpy array):\n",
        "            Tensor: Input image with shape (c, h, w), [0, 1] range.\n",
        "            Numpy: Input image with shape (h, w, c), [0, 1] range.\n",
        "        scale (float): Scale factor. The same scale applies for both height\n",
        "            and width.\n",
        "        antialisaing (bool): Whether to apply anti-aliasing when downsampling.\n",
        "            Default: True.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Output image with shape (c, h, w), [0, 1] range, w/o round.\n",
        "    \"\"\"\n",
        "    if type(img).__module__ == np.__name__:  # numpy type\n",
        "        numpy_type = True\n",
        "        img = torch.from_numpy(img.transpose(2, 0, 1)).float()\n",
        "    else:\n",
        "        numpy_type = False\n",
        "\n",
        "    in_c, in_h, in_w = img.size()\n",
        "    out_h, out_w = math.ceil(in_h * scale), math.ceil(in_w * scale)\n",
        "    kernel_width = 4\n",
        "    kernel = 'cubic'\n",
        "\n",
        "    # get weights and indices\n",
        "    weights_h, indices_h, sym_len_hs, sym_len_he = calculate_weights_indices(\n",
        "        in_h, out_h, scale, kernel, kernel_width, antialiasing)\n",
        "    weights_w, indices_w, sym_len_ws, sym_len_we = calculate_weights_indices(\n",
        "        in_w, out_w, scale, kernel, kernel_width, antialiasing)\n",
        "    # process H dimension\n",
        "    # symmetric copying\n",
        "    img_aug = torch.FloatTensor(in_c, in_h + sym_len_hs + sym_len_he, in_w)\n",
        "    img_aug.narrow(1, sym_len_hs, in_h).copy_(img)\n",
        "\n",
        "    sym_patch = img[:, :sym_len_hs, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
        "    img_aug.narrow(1, 0, sym_len_hs).copy_(sym_patch_inv)\n",
        "\n",
        "    sym_patch = img[:, -sym_len_he:, :]\n",
        "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
        "    img_aug.narrow(1, sym_len_hs + in_h, sym_len_he).copy_(sym_patch_inv)\n",
        "\n",
        "    out_1 = torch.FloatTensor(in_c, out_h, in_w)\n",
        "    kernel_width = weights_h.size(1)\n",
        "    for i in range(out_h):\n",
        "        idx = int(indices_h[i][0])\n",
        "        for j in range(in_c):\n",
        "            out_1[j, i, :] = img_aug[j, idx:idx + kernel_width, :].transpose(\n",
        "                0, 1).mv(weights_h[i])\n",
        "\n",
        "    # process W dimension\n",
        "    # symmetric copying\n",
        "    out_1_aug = torch.FloatTensor(in_c, out_h, in_w + sym_len_ws + sym_len_we)\n",
        "    out_1_aug.narrow(2, sym_len_ws, in_w).copy_(out_1)\n",
        "\n",
        "    sym_patch = out_1[:, :, :sym_len_ws]\n",
        "    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n",
        "    out_1_aug.narrow(2, 0, sym_len_ws).copy_(sym_patch_inv)\n",
        "\n",
        "    sym_patch = out_1[:, :, -sym_len_we:]\n",
        "    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n",
        "    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n",
        "    out_1_aug.narrow(2, sym_len_ws + in_w, sym_len_we).copy_(sym_patch_inv)\n",
        "\n",
        "    out_2 = torch.FloatTensor(in_c, out_h, out_w)\n",
        "    kernel_width = weights_w.size(1)\n",
        "    for i in range(out_w):\n",
        "        idx = int(indices_w[i][0])\n",
        "        for j in range(in_c):\n",
        "            out_2[j, :, i] = out_1_aug[j, :,\n",
        "                                       idx:idx + kernel_width].mv(weights_w[i])\n",
        "\n",
        "    if numpy_type:\n",
        "        out_2 = out_2.numpy().transpose(1, 2, 0)\n",
        "    return out_2\n",
        "\n",
        "\n",
        "def rgb2ycbcr(img, y_only=False):\n",
        "    \"\"\"Convert a RGB image to YCbCr image.\n",
        "\n",
        "    This function produces the same results as Matlab's `rgb2ycbcr` function.\n",
        "    It implements the ITU-R BT.601 conversion for standard-definition\n",
        "    television. See more details in\n",
        "    https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion.\n",
        "\n",
        "    It differs from a similar function in cv2.cvtColor: `RGB <-> YCrCb`.\n",
        "    In OpenCV, it implements a JPEG conversion. See more details in\n",
        "    https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): The input image. It accepts:\n",
        "            1. np.uint8 type with range [0, 255];\n",
        "            2. np.float32 type with range [0, 1].\n",
        "        y_only (bool): Whether to only return Y channel. Default: False.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: The converted YCbCr image. The output image has the same type\n",
        "            and range as input image.\n",
        "    \"\"\"\n",
        "    img_type = img.dtype\n",
        "    img = _convert_input_type_range(img)\n",
        "    if y_only:\n",
        "        out_img = np.dot(img, [65.481, 128.553, 24.966]) + 16.0\n",
        "    else:\n",
        "        out_img = np.matmul(\n",
        "            img, [[65.481, -37.797, 112.0], [128.553, -74.203, -93.786],\n",
        "                  [24.966, 112.0, -18.214]]) + [16, 128, 128]\n",
        "    out_img = _convert_output_type_range(out_img, img_type)\n",
        "    return out_img\n",
        "\n",
        "\n",
        "def bgr2ycbcr(img, y_only=False):\n",
        "    \"\"\"Convert a BGR image to YCbCr image.\n",
        "\n",
        "    The bgr version of rgb2ycbcr.\n",
        "    It implements the ITU-R BT.601 conversion for standard-definition\n",
        "    television. See more details in\n",
        "    https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion.\n",
        "\n",
        "    It differs from a similar function in cv2.cvtColor: `BGR <-> YCrCb`.\n",
        "    In OpenCV, it implements a JPEG conversion. See more details in\n",
        "    https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): The input image. It accepts:\n",
        "            1. np.uint8 type with range [0, 255];\n",
        "            2. np.float32 type with range [0, 1].\n",
        "        y_only (bool): Whether to only return Y channel. Default: False.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: The converted YCbCr image. The output image has the same type\n",
        "            and range as input image.\n",
        "    \"\"\"\n",
        "    img_type = img.dtype\n",
        "    img = _convert_input_type_range(img)\n",
        "    if y_only:\n",
        "        out_img = np.dot(img, [24.966, 128.553, 65.481]) + 16.0\n",
        "    else:\n",
        "        out_img = np.matmul(\n",
        "            img, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786],\n",
        "                  [65.481, -37.797, 112.0]]) + [16, 128, 128]\n",
        "    out_img = _convert_output_type_range(out_img, img_type)\n",
        "    return out_img\n",
        "\n",
        "\n",
        "def ycbcr2rgb(img):\n",
        "    \"\"\"Convert a YCbCr image to RGB image.\n",
        "\n",
        "    This function produces the same results as Matlab's ycbcr2rgb function.\n",
        "    It implements the ITU-R BT.601 conversion for standard-definition\n",
        "    television. See more details in\n",
        "    https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion.\n",
        "\n",
        "    It differs from a similar function in cv2.cvtColor: `YCrCb <-> RGB`.\n",
        "    In OpenCV, it implements a JPEG conversion. See more details in\n",
        "    https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): The input image. It accepts:\n",
        "            1. np.uint8 type with range [0, 255];\n",
        "            2. np.float32 type with range [0, 1].\n",
        "\n",
        "    Returns:\n",
        "        ndarray: The converted RGB image. The output image has the same type\n",
        "            and range as input image.\n",
        "    \"\"\"\n",
        "    img_type = img.dtype\n",
        "    img = _convert_input_type_range(img) * 255\n",
        "    out_img = np.matmul(img, [[0.00456621, 0.00456621, 0.00456621],\n",
        "                              [0, -0.00153632, 0.00791071],\n",
        "                              [0.00625893, -0.00318811, 0]]) * 255.0 + [\n",
        "                                  -222.921, 135.576, -276.836\n",
        "                              ]  # noqa: E126\n",
        "    out_img = _convert_output_type_range(out_img, img_type)\n",
        "    return out_img\n",
        "\n",
        "\n",
        "def ycbcr2bgr(img):\n",
        "    \"\"\"Convert a YCbCr image to BGR image.\n",
        "\n",
        "    The bgr version of ycbcr2rgb.\n",
        "    It implements the ITU-R BT.601 conversion for standard-definition\n",
        "    television. See more details in\n",
        "    https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion.\n",
        "\n",
        "    It differs from a similar function in cv2.cvtColor: `YCrCb <-> BGR`.\n",
        "    In OpenCV, it implements a JPEG conversion. See more details in\n",
        "    https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): The input image. It accepts:\n",
        "            1. np.uint8 type with range [0, 255];\n",
        "            2. np.float32 type with range [0, 1].\n",
        "\n",
        "    Returns:\n",
        "        ndarray: The converted BGR image. The output image has the same type\n",
        "            and range as input image.\n",
        "    \"\"\"\n",
        "    img_type = img.dtype\n",
        "    img = _convert_input_type_range(img) * 255\n",
        "    out_img = np.matmul(img, [[0.00456621, 0.00456621, 0.00456621],\n",
        "                              [0.00791071, -0.00153632, 0],\n",
        "                              [0, -0.00318811, 0.00625893]]) * 255.0 + [\n",
        "                                  -276.836, 135.576, -222.921\n",
        "                              ]  # noqa: E126\n",
        "    out_img = _convert_output_type_range(out_img, img_type)\n",
        "    return out_img\n",
        "\n",
        "\n",
        "def _convert_input_type_range(img):\n",
        "    \"\"\"Convert the type and range of the input image.\n",
        "\n",
        "    It converts the input image to np.float32 type and range of [0, 1].\n",
        "    It is mainly used for pre-processing the input image in colorspace\n",
        "    convertion functions such as rgb2ycbcr and ycbcr2rgb.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): The input image. It accepts:\n",
        "            1. np.uint8 type with range [0, 255];\n",
        "            2. np.float32 type with range [0, 1].\n",
        "\n",
        "    Returns:\n",
        "        (ndarray): The converted image with type of np.float32 and range of\n",
        "            [0, 1].\n",
        "    \"\"\"\n",
        "    img_type = img.dtype\n",
        "    img = img.astype(np.float32)\n",
        "    if img_type == np.float32:\n",
        "        pass\n",
        "    elif img_type == np.uint8:\n",
        "        img /= 255.\n",
        "    else:\n",
        "        raise TypeError('The img type should be np.float32 or np.uint8, '\n",
        "                        f'but got {img_type}')\n",
        "    return img\n",
        "\n",
        "\n",
        "def _convert_output_type_range(img, dst_type):\n",
        "    \"\"\"Convert the type and range of the image according to dst_type.\n",
        "\n",
        "    It converts the image to desired type and range. If `dst_type` is np.uint8,\n",
        "    images will be converted to np.uint8 type with range [0, 255]. If\n",
        "    `dst_type` is np.float32, it converts the image to np.float32 type with\n",
        "    range [0, 1].\n",
        "    It is mainly used for post-processing images in colorspace convertion\n",
        "    functions such as rgb2ycbcr and ycbcr2rgb.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): The image to be converted with np.float32 type and\n",
        "            range [0, 255].\n",
        "        dst_type (np.uint8 | np.float32): If dst_type is np.uint8, it\n",
        "            converts the image to np.uint8 type with range [0, 255]. If\n",
        "            dst_type is np.float32, it converts the image to np.float32 type\n",
        "            with range [0, 1].\n",
        "\n",
        "    Returns:\n",
        "        (ndarray): The converted image with desired type and range.\n",
        "    \"\"\"\n",
        "    if dst_type not in (np.uint8, np.float32):\n",
        "        raise TypeError('The dst_type should be np.float32 or np.uint8, '\n",
        "                        f'but got {dst_type}')\n",
        "    if dst_type == np.uint8:\n",
        "        img = img.round()\n",
        "    else:\n",
        "        img /= 255.\n",
        "    return img.astype(dst_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2edWU7Jn9Mm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def reorder_image(img, input_order='HWC'):\n",
        "    \"\"\"Reorder images to 'HWC' order.\n",
        "\n",
        "    If the input_order is (h, w), return (h, w, 1);\n",
        "    If the input_order is (c, h, w), return (h, w, c);\n",
        "    If the input_order is (h, w, c), return as it is.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): Input image.\n",
        "        input_order (str): Whether the input order is 'HWC' or 'CHW'.\n",
        "            If the input image shape is (h, w), input_order will not have\n",
        "            effects. Default: 'HWC'.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: reordered image.\n",
        "    \"\"\"\n",
        "\n",
        "    if input_order not in ['HWC', 'CHW']:\n",
        "        raise ValueError(\n",
        "            f'Wrong input_order {input_order}. Supported input_orders are '\n",
        "            \"'HWC' and 'CHW'\")\n",
        "    if len(img.shape) == 2:\n",
        "        img = img[..., None]\n",
        "    if input_order == 'CHW':\n",
        "        img = img.transpose(1, 2, 0)\n",
        "    return img\n",
        "\n",
        "\n",
        "def to_y_channel(img):\n",
        "    \"\"\"Change to Y channel of YCbCr.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): Images with range [0, 255].\n",
        "\n",
        "    Returns:\n",
        "        (ndarray): Images with range [0, 255] (float type) without round.\n",
        "    \"\"\"\n",
        "    img = img.astype(np.float32) / 255.\n",
        "    if img.ndim == 3 and img.shape[2] == 3:\n",
        "        img = bgr2ycbcr(img, y_only=True)\n",
        "        img = img[..., None]\n",
        "    return img * 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJwZn3OsRQQV",
        "outputId": "502f1a61-ffa0-4dcf-8c53-84f49b18b80f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-226b428f5a56>:4: DeprecationWarning: Please use `convolve` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
            "  from scipy.ndimage.filters import convolve\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.ndimage.filters import convolve\n",
        "from scipy.special import gamma\n",
        "\n",
        "\n",
        "\n",
        "def estimate_aggd_param(block):\n",
        "    \"\"\"Estimate AGGD (Asymmetric Generalized Gaussian Distribution) paramters.\n",
        "\n",
        "    Args:\n",
        "        block (ndarray): 2D Image block.\n",
        "\n",
        "    Returns:\n",
        "        tuple: alpha (float), beta_l (float) and beta_r (float) for the AGGD\n",
        "            distribution (Estimating the parames in Equation 7 in the paper).\n",
        "    \"\"\"\n",
        "    block = block.flatten()\n",
        "    gam = np.arange(0.2, 10.001, 0.001)  # len = 9801\n",
        "    gam_reciprocal = np.reciprocal(gam)\n",
        "    r_gam = np.square(gamma(gam_reciprocal * 2)) / (\n",
        "        gamma(gam_reciprocal) * gamma(gam_reciprocal * 3))\n",
        "\n",
        "    left_std = np.sqrt(np.mean(block[block < 0]**2))\n",
        "    right_std = np.sqrt(np.mean(block[block > 0]**2))\n",
        "    gammahat = left_std / right_std\n",
        "    rhat = (np.mean(np.abs(block)))**2 / np.mean(block**2)\n",
        "    rhatnorm = (rhat * (gammahat**3 + 1) *\n",
        "                (gammahat + 1)) / ((gammahat**2 + 1)**2)\n",
        "    array_position = np.argmin((r_gam - rhatnorm)**2)\n",
        "\n",
        "    alpha = gam[array_position]\n",
        "    beta_l = left_std * np.sqrt(gamma(1 / alpha) / gamma(3 / alpha))\n",
        "    beta_r = right_std * np.sqrt(gamma(1 / alpha) / gamma(3 / alpha))\n",
        "    return (alpha, beta_l, beta_r)\n",
        "\n",
        "\n",
        "def compute_feature(block):\n",
        "    \"\"\"Compute features.\n",
        "\n",
        "    Args:\n",
        "        block (ndarray): 2D Image block.\n",
        "\n",
        "    Returns:\n",
        "        list: Features with length of 18.\n",
        "    \"\"\"\n",
        "    feat = []\n",
        "    alpha, beta_l, beta_r = estimate_aggd_param(block)\n",
        "    feat.extend([alpha, (beta_l + beta_r) / 2])\n",
        "\n",
        "    # distortions disturb the fairly regular structure of natural images.\n",
        "    # This deviation can be captured by analyzing the sample distribution of\n",
        "    # the products of pairs of adjacent coefficients computed along\n",
        "    # horizontal, vertical and diagonal orientations.\n",
        "    shifts = [[0, 1], [1, 0], [1, 1], [1, -1]]\n",
        "    for i in range(len(shifts)):\n",
        "        shifted_block = np.roll(block, shifts[i], axis=(0, 1))\n",
        "        alpha, beta_l, beta_r = estimate_aggd_param(block * shifted_block)\n",
        "        # Eq. 8\n",
        "        mean = (beta_r - beta_l) * (gamma(2 / alpha) / gamma(1 / alpha))\n",
        "        feat.extend([alpha, mean, beta_l, beta_r])\n",
        "    return feat\n",
        "\n",
        "\n",
        "def niqe(img,\n",
        "         mu_pris_param,\n",
        "         cov_pris_param,\n",
        "         gaussian_window,\n",
        "         block_size_h=96,\n",
        "         block_size_w=96):\n",
        "    \"\"\"Calculate NIQE (Natural Image Quality Evaluator) metric.\n",
        "\n",
        "    Ref: Making a \"Completely Blind\" Image Quality Analyzer.\n",
        "    This implementation could produce almost the same results as the official\n",
        "    MATLAB codes: http://live.ece.utexas.edu/research/quality/niqe_release.zip\n",
        "\n",
        "    Note that we do not include block overlap height and width, since they are\n",
        "    always 0 in the official implementation.\n",
        "\n",
        "    For good performance, it is advisable by the official implemtation to\n",
        "    divide the distorted image in to the same size patched as used for the\n",
        "    construction of multivariate Gaussian model.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): Input image whose quality needs to be computed. The\n",
        "            image must be a gray or Y (of YCbCr) image with shape (h, w).\n",
        "            Range [0, 255] with float type.\n",
        "        mu_pris_param (ndarray): Mean of a pre-defined multivariate Gaussian\n",
        "            model calculated on the pristine dataset.\n",
        "        cov_pris_param (ndarray): Covariance of a pre-defined multivariate\n",
        "            Gaussian model calculated on the pristine dataset.\n",
        "        gaussian_window (ndarray): A 7x7 Gaussian window used for smoothing the\n",
        "            image.\n",
        "        block_size_h (int): Height of the blocks in to which image is divided.\n",
        "            Default: 96 (the official recommended value).\n",
        "        block_size_w (int): Width of the blocks in to which image is divided.\n",
        "            Default: 96 (the official recommended value).\n",
        "    \"\"\"\n",
        "    assert img.ndim == 2, (\n",
        "        'Input image must be a gray or Y (of YCbCr) image with shape (h, w).')\n",
        "    # crop image\n",
        "    h, w = img.shape\n",
        "    num_block_h = math.floor(h / block_size_h)\n",
        "    num_block_w = math.floor(w / block_size_w)\n",
        "    img = img[0:num_block_h * block_size_h, 0:num_block_w * block_size_w]\n",
        "\n",
        "    distparam = []  # dist param is actually the multiscale features\n",
        "    for scale in (1, 2):  # perform on two scales (1, 2)\n",
        "        mu = convolve(img, gaussian_window, mode='nearest')\n",
        "        sigma = np.sqrt(\n",
        "            np.abs(\n",
        "                convolve(np.square(img), gaussian_window, mode='nearest') -\n",
        "                np.square(mu)))\n",
        "        # normalize, as in Eq. 1 in the paper\n",
        "        img_nomalized = (img - mu) / (sigma + 1)\n",
        "\n",
        "        feat = []\n",
        "        for idx_w in range(num_block_w):\n",
        "            for idx_h in range(num_block_h):\n",
        "                # process ecah block\n",
        "                block = img_nomalized[idx_h * block_size_h //\n",
        "                                      scale:(idx_h + 1) * block_size_h //\n",
        "                                      scale, idx_w * block_size_w //\n",
        "                                      scale:(idx_w + 1) * block_size_w //\n",
        "                                      scale]\n",
        "                feat.append(compute_feature(block))\n",
        "\n",
        "        distparam.append(np.array(feat))\n",
        "        # TODO: matlab bicubic downsample with anti-aliasing\n",
        "        # for simplicity, now we use opencv instead, which will result in\n",
        "        # a slight difference.\n",
        "        if scale == 1:\n",
        "            h, w = img.shape\n",
        "            img = cv2.resize(\n",
        "                img / 255., (w // 2, h // 2), interpolation=cv2.INTER_LINEAR)\n",
        "            img = img * 255.\n",
        "\n",
        "    distparam = np.concatenate(distparam, axis=1)\n",
        "\n",
        "    # fit a MVG (multivariate Gaussian) model to distorted patch features\n",
        "    mu_distparam = np.nanmean(distparam, axis=0)\n",
        "    # use nancov. ref: https://ww2.mathworks.cn/help/stats/nancov.html\n",
        "    distparam_no_nan = distparam[~np.isnan(distparam).any(axis=1)]\n",
        "    cov_distparam = np.cov(distparam_no_nan, rowvar=False)\n",
        "\n",
        "    # compute niqe quality, Eq. 10 in the paper\n",
        "    invcov_param = np.linalg.pinv((cov_pris_param + cov_distparam) / 2)\n",
        "    quality = np.matmul(\n",
        "        np.matmul((mu_pris_param - mu_distparam), invcov_param),\n",
        "        np.transpose((mu_pris_param - mu_distparam)))\n",
        "    quality = np.sqrt(quality)\n",
        "\n",
        "    return quality\n",
        "\n",
        "\n",
        "def calculate_niqe(img, crop_border=0, input_order='HWC', convert_to='y'):\n",
        "    \"\"\"Calculate NIQE (Natural Image Quality Evaluator) metric.\n",
        "\n",
        "    Ref: Making a \"Completely Blind\" Image Quality Analyzer.\n",
        "    This implementation could produce almost the same results as the official\n",
        "    MATLAB codes: http://live.ece.utexas.edu/research/quality/niqe_release.zip\n",
        "\n",
        "    We use the official params estimated from the pristine dataset.\n",
        "    We use the recommended block size (96, 96) without overlaps.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): Input image whose quality needs to be computed.\n",
        "            The input image must be in range [0, 255] with float/int type.\n",
        "            The input_order of image can be 'HW' or 'HWC' or 'CHW'. (BGR order)\n",
        "            If the input order is 'HWC' or 'CHW', it will be converted to gray\n",
        "            or Y (of YCbCr) image according to the ``convert_to`` argument.\n",
        "        crop_border (int): Cropped pixels in each edge of an image. These\n",
        "            pixels are not involved in the metric calculation.\n",
        "        input_order (str): Whether the input order is 'HW', 'HWC' or 'CHW'.\n",
        "            Default: 'HWC'.\n",
        "        convert_to (str): Whether coverted to 'y' (of MATLAB YCbCr) or 'gray'.\n",
        "            Default: 'y'.\n",
        "\n",
        "    Returns:\n",
        "        float: NIQE result.\n",
        "    \"\"\"\n",
        "\n",
        "    # we use the official params estimated from the pristine dataset.\n",
        "    niqe_pris_params = np.load('/content/niqe_pris_params.npz')\n",
        "    mu_pris_param = niqe_pris_params['mu_pris_param']\n",
        "    cov_pris_param = niqe_pris_params['cov_pris_param']\n",
        "    gaussian_window = niqe_pris_params['gaussian_window']\n",
        "\n",
        "    img = img.astype(np.float32)\n",
        "    if input_order != 'HW':\n",
        "        img = reorder_image(img, input_order=input_order)\n",
        "        if convert_to == 'y':\n",
        "            img = to_y_channel(img)\n",
        "        elif convert_to == 'gray':\n",
        "            img = cv2.cvtColor(img / 255., cv2.COLOR_BGR2GRAY) * 255.\n",
        "        img = np.squeeze(img)\n",
        "\n",
        "    if crop_border != 0:\n",
        "        img = img[crop_border:-crop_border, crop_border:-crop_border]\n",
        "\n",
        "    niqe_result = niqe(img, mu_pris_param, cov_pris_param, gaussian_window)\n",
        "\n",
        "    return niqe_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u8PVC_AddKm",
        "outputId": "05775414-d1ad-4709-cf81-29307e70d68e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[5.15783913]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img = cv2.imread('/content/enhanced_image.jpg')\n",
        "calculate_niqe(img, crop_border=0, input_order='HWC', convert_to='y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH6H6zbknqA7",
        "outputId": "7179b4b3-ee9f-4321-da54-a7e384f98991"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[4.13948372]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img = cv2.imread('/content/result/HD-wallpaper-saitama-one-punch-man-manga-hero-sensei-anime-jpg_colored_output.jpg')\n",
        "calculate_niqe(img, crop_border=0, input_order='HWC', convert_to='y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "iGOdI5Hfn4Od",
        "outputId": "b1a6edd3-3c84-4d3c-f5b4-fb7f5ba791ee"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('/content/saitama-one-punch-man.png')\n",
        "calculate_niqe(img, crop_border=0, input_order='HWC', convert_to='y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFfVIK-7oGZH"
      },
      "source": [
        "# PSNR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFi2tps7oC9i"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def compute_psnr(img1_path, img2_path):\n",
        "    # Read images\n",
        "    img1 = cv2.imread(img1_path)\n",
        "    img2 = cv2.imread(img2_path)\n",
        "\n",
        "    # Convert images to float32\n",
        "    img1 = img1.astype(np.float32)\n",
        "    img2 = img2.astype(np.float32)\n",
        "\n",
        "    # Compute MSE (Mean Squared Error)\n",
        "    mse = np.mean((img1 - img2) ** 2)\n",
        "\n",
        "    # Calculate PSNR\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "    max_pixel = 255.0\n",
        "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "\n",
        "    return psnr\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFqczJOZoVSw",
        "outputId": "d0a7c875-9df4-4a1d-8fe8-f05c2a2c3306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PSNR value: 4.68 dB\n"
          ]
        }
      ],
      "source": [
        "# Paths to the images\n",
        "img1_path = '/content/HD-wallpaper-saitama-one-punch-man-manga-hero-sensei-anime.jpg'\n",
        "img2_path = '/content/enhanced_image.jpg'\n",
        "\n",
        "# Compute PSNR\n",
        "psnr_value = compute_psnr(img1_path, img2_path)\n",
        "print(f'PSNR value: {psnr_value:.2f} dB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOHZ4rThocUj",
        "outputId": "c48563a5-8f76-4fe5-894c-dcbf61e4775a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PSNR value: 17.12 dB\n"
          ]
        }
      ],
      "source": [
        "# Paths to the images\n",
        "img1_path = '/content/HD-wallpaper-saitama-one-punch-man-manga-hero-sensei-anime.jpg'\n",
        "img2_path = '/content/result/HD-wallpaper-saitama-one-punch-man-manga-hero-sensei-anime-jpg_colored_output.jpg'\n",
        "\n",
        "# Compute PSNR\n",
        "psnr_value = compute_psnr(img1_path, img2_path)\n",
        "print(f'PSNR value: {psnr_value:.2f} dB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "ZcNf2RecogUy",
        "outputId": "c86468bf-9c25-41dc-fa22-092d060718b9"
      },
      "outputs": [],
      "source": [
        "# Paths to the images\n",
        "img1_path = '/content/HD-wallpaper-saitama-one-punch-man-manga-hero-sensei-anime.jpg'\n",
        "img2_path = '/content/saitama-one-punch-man.png'\n",
        "\n",
        "# Compute PSNR\n",
        "psnr_value = compute_psnr(img1_path, img2_path)\n",
        "print(f'PSNR value: {psnr_value:.2f} dB')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Iv0y2BKLXRvn",
        "Qvw-3FTeXudd",
        "3ZcJ9fjtXdYM",
        "lmNC-RoUcoZC",
        "PLknY7SoayJI",
        "Z7qX202KbJv8",
        "mZzjRYTXbk6h",
        "27hFT2chb2uT",
        "qy85IUHnhCGd"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
